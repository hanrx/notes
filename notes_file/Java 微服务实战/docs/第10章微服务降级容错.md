# [README](../README.md "回到 README")
# [目录](本书的组织结构.md "回到 目录")

# 第10章 微服务降级容错

## 10.1 初始Hystrix

Hystrix是Netflix开源技术栈中的又一员猛将，其主要用于在微服务中实现容错和降级。

### 10.1.1 为什么要使用Hystrix

来看三个场景。
场景一：如图10-1所示，其中圆圈代表用户，假设有两台服务器server1和server2，在server1上注册了两个服务service1和service2，在service2上注册了一个服务service3。假设service3服务响应缓慢，service1调用service3时，一直在等待响应，那么在高并发的情况下，service1处很快就会达到并发处理请求的阈值而宕机，这时候，不只是service1不可用，server1上的service2也不可用了。
![](images/10.1.1.1.png)
    
场景二：如图10-2所示，假设有三台服务器server1、server2和server3，三台服务器上分别注册了一个服务，假设为service1、service2和service3。用户发起一个请求来调用service1，那么service1需要调用service3才能处理完这个请求。假设service3服务响应缓慢，service2调用service3时，一直在等待响应，那么在高并发的情况下，server2处很快就会达到并发处理请求的阈值而宕机，这个时候，由于service2响应缓慢，service1上积压的请求也会急剧增加，导致server1处也可能达到请求并发处理的阈值而宕机，这样就造成了级联失败，严重的话，可能会造成雪崩。
![](images/10.1.1.2.png)

场景三：假设场景一中的service3崩了，那么service3服务恢复到正常状态可能会花比较长的一段时间，也可能需要人工来恢复。

这几个场景都是很危险的，尤其是场景二，服务雪崩绝对是企业的大灾难。为了防止出现这些问题，Netflix又退出了一员大将：Hystrix！！!下面我们就来看看Hystrix是怎样解决这些问题的。

### 10.1.2 Hystrix工作原理

首先来看一下Hystrix官网提供的一张图。
![](images/10.1.2.1.png)
![](images/10.1.2.2.png)
    
如图10-3所示，Hystrix提供了一种特殊的线程池模型，**它为每一个服务提供一个线程池**，其中，线程池的数量是可以配置的。这种模型其实是**舱壁模式的一种实现**，它将每个依赖服务隔离开来，隔离后，延时的被调用服务只会耗尽自己的线程池，之后进入失败状态；如果不隔离，可能会耗尽整个Tomcat的线程池，导致整个服务器挂掉。这样就解决了场景一的问题，service1的线程池耗尽后，server1的线程池不会耗尽，service2依旧可用。

除了线程池模型，**Hystrix还提供了一种fallback模式，允许我们控制被调用服务的响应延时时间**，对超出这个时间的请求，可用使之**快速失败**（立即抛出运行时异常结束流程），其也可以返回我们指定的一个默认的响应。这样就解决了场景二的问题，service3响应慢，要么立即抛出异常，要么返回一个默认值，这样service2上的请求数量不会积压，从而service1上也就不会积压，有效地防止了级联失败。

除了上述的两种除了方式，Hystrix还提供了一种机制：**熔断**。下面简单说一下整个熔断的过程：
在一定的时间（该值可以配置）内，假设对服务的请求的QPS达到一定阈值（该阈值可配置），同时，这些请求的错误率达到一定阈值（该阈值可配置），断路器打开。注意，这是熔断发生的条件。
断路器打开之后，断路器将短路后续所有经过该熔断器的请求，这些请求直接走fallback逻辑。
经过一定时间（即“**休眠窗口**”，该阈值可配置），断路器处于半开状态，允许后续的第一个请求对服务进行调用，若请求失败，断路器继续打开，再等待一个“休眠窗口”后再处于半开状态，循环往复；若请求成功，断路器关闭。

断路器模式可以有效地防止场景二的问题；也可以处理场景三的问题，只要“休眠窗口”的值配置合理，服务就有可能快速恢复，且不需要人工干预。

### 10.1.3 Hystrix执行流程

首先我们依旧来看一下Hystrix官网给出的流程图。
![](images/10.1.3.1.png)

如图10-4所示，Hystrix的执行流程如下：
1. 构建HystrixCommand或者HystrixObservableCommand对象。
2. 执行命令进行服务调用（execute()、queue()、observe()）。
3. 如果请求结果缓存这个特性被启用，并且缓存命中，立即返回缓存中的值。
4. 如果缓存特性没有启用或者没有命中缓存，则检查断路器状态，确定请求线路是否是开路，如果请求路线是开路，则直接执行fallback。
5. 如果断路器没开，则看为当前被调用服务配置的线程池和请求队列是否已经满了，如果满了，则直接执行fallback。
6. 如果没满，执行HystrixCommand.run()或者HystrixObservableCommand.construct();如果这两个方法执行超时或者执行失败，则执行fallback;如果正常结束，返回响应。

在整个过程中，Hystrix会采集一些数据，以进行后续的熔断操作或者对服务进行监控。这些数据可以在Hystrix-Dashboard上显示。

下面我们编写代码来看一下怎样使用Hystrix实现服务容错。依旧使用两个服务myserviceD和hystrixService，myserviceD依然使用第9章中创建的myserviceD，其仅仅提供一个接口“http://ip:port/myserviceD/getUser?username=xxx”供其他服务进行调用。

## 10.2 使用Hystrix实现服务降级容错

### 10.2.1 搭建项目框架

创建一个新的服务hystrixservice，项目结构如图10-5所示。
![](images/10.2.1.1.png)

其中，pom.xml文件的内容如下：
![](images/10.2.1.2.png)
![](images/10.2.1.3.png)
除了熟悉的spring-boot-starter-web、Swagger、Lombok、fastjson及async-http-client的依赖外，还引入了以下hystrix-core依赖，以提供Hystrix的核心接口。

引入依赖之后，创建主类，代码如下：
![](images/10.2.1.4.png)
依旧是熟悉的启动主类。

项目框架搭建完成之后，编写代码实现服务降级容错功能。

### 10.2.2 创建AsyncHttpClient调用实体类

服务通信框架依然使用AsyncHttpClient，需要创建一个AsyncHttpClient单例Bean，代码如下：
![](images/10.2.2.1.png)

### 10.2.3 服务通信框架集成服务降级容错功能

为了实现服务降级容错功能，需要使用HystrixCommand实现类来包裹服务通信框架的调用行为，具体代码如下：
![](images/10.2.3.1.png)
![](images/10.2.3.2.png)

该类继承了com.netflix.hystrix.HystrixCommand，且指定了泛型参数为com.ning.http.client.Response，该值也是run()和getFallBack()方法的返回值。run()方法时正常的调用的方法，在该方法内，我们使用asyncHttpClient执行对myserviceD的调用；getFallBack()方法是fallback模式的方法，就是run()方法执行失败或者超时时调用的方法，其可以返回一个自定义的com.ning.http.client.Response实例，也可以抛出异常，让请求快速失败。

关于该类还有一个值得注意的事情，就是**必须要有构造器，而且在构造器中必须调用super(setter)方法**，在该方法中，会为当前的MyHystrixCommand实例初始化一系列的配置信息，例如commandKey等。

下面，编写一个集成服务降级容错功能的服务通信类，代码如下：
![](images/10.2.3.3.png)

我们在这里定义了一个execute方法，在该方法中，先创建了一个com.netflix.hystrix.HystrixCommand.Setter实例，之后为该实例设置了commandGroupKey、commandKey、超时时间（这里设为3s）以及为hystrixservice服务调用myserviceD创建的线程池的核心线程数coreSize。

commandGroupKey通常被设为servicename；而如果没有自己指定threadPoolKey的值，则其通常会取commandGroupKey的值；commandKey通常会以方法名命名，为了防止多个服务的方法重名，通常会使用servicename前缀。

该setter最后会用来创建MyHystrixCommand实例时，为其设置属性值，具体看一下com.microservice.hystrixservice.config.MyHystrixCommand的构造器的super(setter)的源码，这里不再赘述。
    
最后，我们来验证降级容错功能。

### 10.2.4 验证服务降级容错功能

首先，创建一个模型类来接受myserviceD被调用接口的返回值。代码如下：
![](images/10.2.4.1.png)

然后编写一个controller进行调用。
![](images/10.2.4.2.png)
![](images/10.2.4.3.png)

代码编写完成后，部署服务进行测试。启动myserviceD服务，然后启动hystrixservice服务，进入Swagger进行测试！！！这时候，由于Hystrix的超时时间是3s，而myserviceD相应也比较快，所以会返回正常的结果。我们对myserviceD的getUser方法进行一点修改，如下：
![](images/10.2.4.4.png)

该方法在执行过程中首先会睡5s，之后才会执行后续逻辑。这样的话，已经超出了Hystrix的超时时间（3s），所以会执行fallback逻辑，直接抛出异常。在response body中会返回如下信息：
![](images/10.2.4.5.png)
其中message是我们自己指定的报错信息。

最后，笔者要指定一个坑（是个隐秘的坑），就是**Hystrix的超时时间和AsyncHttpClient的请求超时时间的设置，请记住一点，以二者的最小值为准**。在本例中，笔者为Hystrix设置的超时时间是3s，而AsyncHttpClient的requestTimeout为50s。所以整个请求的超时时间是3s，而如果为Hystrix设置的超时时间是30s，AsyncHttpClient的requestTimeout为20s，则整个请求的超时时间为20s。由于AsyncHttpClient的调用是在run()方法中进行的，当整个请求超时20s时，会出现AsyncHttpClient调用超时错误，也就是run()执行错误，所以会马上执行getFallback()方法。

## 10.3 搭建Hystrix监控系统

如图10-4所示，Hystrix在执行过程中，会监控记录一些有用的数据，至于记录了哪些数据，HystrixDashboard会向我们展示。使用这些数据我们可以更好地来设置Hystrix的众多属性，使系统达到最优的状态。**Hystrix将最近10s内（当然，这个值也是可以配置的）的数据存储在内存中的一个滚动桶模型中（该模型有10个桶，每个桶记录1s的数据）**，这里引用Hystrix官网的一张图片，如图10-6所示。
![](images/10.3.1.png)

从这个桶中我们只能拿到最近10s的数据，再往前的数据只能通过分析HystrixRequestLog来获得了。

### 10.3.1 使用Hystrix-Metrics-Event-Stream发布监控信息

想使用Hystrix-Metrics将统计信息发布出来，只需要在hystrixservice服务中进行如下两步操作即可。

第一步，在pom.xml中引入hystrix-metrics-event-stream依赖，如下：
![](images/10.3.1.1.png)

第二步，创建HystrixMetricsStreamServlet实例并注册，代码如下：
![](images/10.3.1.2.png)

在HystrixServiceConfig中添加了两个Bean：**一个是HystrixMetricsStreamServlet；另一个是用来注册Servlet的ServletRegistrationBean,并且指定了注册的Servlet拦截的urlMappings**。

至此代码部分就完成了，只要Hystrix-Dashboard连接一直连着，Hystrix-Metrics-Event-Stream就会不断地向客户端以text/event-stream的形式推送计数结果（metrics）。

### 10.3.2 使用Hystrix-Dashboard展示监控信息

使用Hystrix-Dashboard有三种姿势：
* 下载Hystrix-Dashboard-#.#.#.war包，并放到Tomcat中，之后以Web项目启动。
* 下载Hystrix-Dashboard的fatjar，之后直接以jar包方式启动。
* 下载Hystrix-Dashboard的Docker镜像，以Docker镜像启动。

其中，第二种最简单，这里使用第二种方式。

第一步，在本机下载standalone-hystrix-dashboard-1.5.3-all.jar，下载地址为：https://github.com/kennedyoliveira/standalong-hystrix-dashboard 。

第二步，将下载好的jar包复制到10.211.55.13（笔者的一台centos7的虚拟机）。
![](images/10.3.2.1.png)

第三步，启动Hystrix-Dashboard。
![](images/10.3.2.2.png)

其中，serverPort的默认值是7979，bingAddress的默认值是localhost。

第四步，在浏览器中输入：http://10.211.55.13:7979/hystrix-dashboard/，出现小熊页面就表明正确运行了。

第五步，在小熊页面按照图10-7所示的流程进行操作。
![](images/10.3.2.3.png)

首先在输入框输入hystrixservice的监控地址：http://host:port/hystrix.stream，然后单击Add Stream按钮，最后单击Monitor Streams按钮即可。这里注意，如果host用localhost或者127.0.0.1不行的话，需要使用真实IP。单击Monitor Stream按钮之后，会看到如图10-8所示的页面。
![](images/10.3.2.4.png)

可以看到，**Hystrix监控了每个commandKey和threadPool**。这里通过Hystrix官方的一张图片来看一下Circuit的仪表盘中的各个值代表什么意思（其实可以根据Hystrix-Dashboard页面右上方的颜色来判断每一个参数代表什么意思）。
![](images/10.3.2.5.png)

读者可以使用Swagger进行测试，看看Hystrix-Dashboard上的数据变化。该页面是动态刷新的，但是时间久了也会卡死，如果页面没有刷新，手动刷新一下。

### 10.3.3 使用Turbine聚合监控信息

**Turbine是Netflix开源技术栈中的又一力作，其主要用于聚合数据**。在Hystrix中，其主要用于将相同的commandKey、Threadpool等监控数据进行聚合。

具体可以看一下这个场景：假设现在有两个服务hystrixservice和hystrixservice2（hystrixservice2的代码从hystrixservice复制而来）都要访问myserviceD服务的getUser方法。那么在Hystrix-Dashboard中myserviceD：getUser这个commandKey以及myserviceD这个threadPool会分开在两个流中展示，如图10-10所示。
![](images/10.3.3.1.png)
![](images/10.3.3.2.png)

实际上我们只需要统计myserviceD服务的getUser方法的处理能力，与谁调用无关，所以希望让相同的commandKey或相同的threadPool展示在一个仪表盘中，这样才可以看出这个方法或者这个服务的整体响应能力。这个时候就需要使用Tuibine将这些数据汇聚起来。使用Turbine需要执行如下步骤：
    
第一步，从本地下载Tomcat和Turbine的war包，之后上传到10.211.55.13。
![](images/10.3.3.3.png)
Turbine的war包下载地址为：https://mvnrepository.com/artifact/com.netflix.turbine/turbine-web/1.0.0 。

第二步，在10.211.55.13上安装Tomcat并部署Turbine。
![](images/10.3.3.4.png)

在/etc/profile中配置Tomcat环境变量。
![](images/10.3.3.5.png)

配置完成之后，使配置文件生效，然后部署Turbine。
![](images/10.3.3.6.png)

在Host标签内添加相关内容。
![](images/10.3.3.7.png)

最后，启动Tomcat。
![](images/10.3.3.8.png)

此时，turbine-web-1.0.0.war就会完成自动解压。我们在浏览器中输入http://10.211.55.13:8080/tuibine-web-1.0.0/turbine.stream，如果现实“HTTP Status 404-Cluster not found”，则表示turbine-web-1.0.0.war部署成功！！！当然也可以先动手解压turbine-web-1.0.0.war,执行完下边的第三步后再启动Tomcat。


第三步，配置Turbine。
![](images/10.3.3.9.png)

配置以下4项： 
![](images/10.3.3.10.png)
说明：
* turbine.aggregator.clusterConfig是Turbine汇聚数据的单位，这里只配置了一个default，那么Turbine会对所有的项目进行汇聚。
* turbine.ConfigPropertyBaseDiscovery.default.instances与turbine.instanceUrlSuffix共同构建获取Hystix-Stream的url，这里是两个：http://xx.xx.xx.xxx:8081/hystrix.stream 和http://10.211.55.13:8081/hystrix.stream 。最后，Turbine会将这两个Hystrix-Stream汇聚起来。其中xx.xx.xx.xxx和10.211.55.13分别部署hystrixservice和hystrixservice2。

第四步，重新启动Tomcat。
![](images/10.3.3.11.png)

第五步，在Hystrix-Dashboard上添加Turbine-Stream，如图10-11所示。
![](images/10.3.3.12.png)

单击Monitor Streams按钮之后，显示结果如图10-12所示。
![](images/10.3.3.13.png)

Hosts变为了2，PoolSize变为了40,这说明hystrixservice和hystrixservice2两个服务汇聚成功。
    
至此，一套Hystrix监控系统就搭建成功了！！！当然，Turbine的聚合也是实时的。

这里笔者还遗留了一个问题，就是在实际开发中，我们不可能每添加一个服务或机器就去修改Turbine的config.properties配置文件，而是需要对Turbine的代码进行二次开发，通过服务发现来做这个事。

## 10.4 再学一招：Hystrix常用配置

在本章的“再学一招”部分，介绍一些常用的Hystrix配置。

### 10.4.1 设置配置参数的两种方法

第一种，像本章中的例子一样，**直接使用代码来设置**，例如设置超时时间：
![](images/10.4.1.1.png)

第二种，当项目使用Archaius进行服务配置的时候，可以直接在配置文件中设置，这是指定某一个commandKey的超时时间为5s，如果将上边的HystrixCommandKey换成default，那就是将全部的HystrixCommandKey的超时时间都设为5s。这样设置了之后，Hystrix在执行的时候默认会去Archaius读取key为上述配置的value，所以这些key要写对而不能自己定义。
![](images/10.4.1.2.png)

通常情况下，推荐使用第二种，因为这种方法将配置信息集中起来并且动态管理了。

### 10.4.2 常见配置项的配置方式


1.执行策略
![](images/10.4.2.1.png)

可选值：THREAD、SEMAPHORE。**使用线程池模型的好处在10.1.2节中已经讲了。坏处就是占用太多的内存**，但是在绝大多数情况下，Netflix更偏向于使用线程池来隔离依赖服务，因为其带来的额外开销是可以接受的，并且它能支持包括超时在内的所有功能。通常不需要使用信号量。

2.超时时间
![](images/10.4.2.2.png)

3.是否启用断路器
![](images/10.4.2.3.png)

4.实现断路的最小请求量
![](images/10.4.2.4.png)

5.实现断路的最小错误率
![](images/10.4.2.5.png)

6.实现断路的最小时间（Hystrix-Metrics记录时间的统计数据）
![](images/10.4.2.6.png)

7.断路器打开后多久开始处于半开状态
![](images/10.4.2.7.png)

这里注意，产生断路的条件是在6中设置的时间内（10s）至少要有4中设置的请求数（20个），并且错误率达到5中设置（50%）,才会发生断路。（当然，前提是3中的设置为true），在断路后达到在7中设置的时间（5s）后开始处于半开状态。

8.是否启用请求缓存
![](images/10.4.2.8.png)

通过使用request范围内的缓存来防止重复请求。

9.线程池的核心线程数
![](images/10.4.2.9.png)

10.队列保存的最多的元素个数
![](images/10.4.2.10.png)

-1：代表使用SynchronousQueue，那么当coreSize满了之后，再来的请求就直接回绝了；正整数：代表使用LinkedBlockingQueue，并且队列最大值为指定的正整数。

11.队列动态变化大型
![](images/10.4.2.11.png)

注意：当10中的设置为-1时，该项不起作用；当在10中设置的值为正数时，例如100，线程池就会创建一个长度为100的队列，且该队列的大小不能动态变化，所以如果不考虑11配置项的话，当同时过来coreSize+100个请求时，此时再过来的请求就会被回绝；如果配置了11配置项，假设为5，那么超过coresize+5个请求时，再来的请求就会被回绝。起到了一个相当于动态更改队列大小的作用。

以上就是最常用的11个配置项，其他使用默认值就可以了。


# [README](../README.md "回到 README")
# [目录](本书的组织结构.md "回到 目录")